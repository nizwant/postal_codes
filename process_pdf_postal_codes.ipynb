{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c567da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_multipage_table_to_csv(\n",
    "    pdf_path,\n",
    "    pages=\"all\",\n",
    "    flavor=\"lattice\",\n",
    "    output_file=\"merged_table.csv\",\n",
    "    keep_header_from_first_page=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parse a table that spans multiple pages and merge into a single CSV file\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pdf_path : str\n",
    "        Path to the PDF file\n",
    "    pages : str\n",
    "        Pages to parse. Can be 'all', '1,2,3', or '1-3'\n",
    "    flavor : str\n",
    "        'lattice' for tables with lines, 'stream' for tables without lines\n",
    "    output_file : str\n",
    "        Name of the output CSV file\n",
    "    keep_header_from_first_page : bool\n",
    "        If True, uses header from first page only and skips headers on subsequent pages\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame: The merged table\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Read PDF tables\n",
    "        print(f\"Reading tables from {pdf_path}...\")\n",
    "        tables = camelot.read_pdf(\n",
    "            pdf_path, pages=pages, flavor=flavor, table_areas=[\"28,813,567,27\"]\n",
    "        )\n",
    "\n",
    "        if len(tables) == 0:\n",
    "            print(\"No tables found in the PDF\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Found {len(tables)} table(s) across pages\")\n",
    "\n",
    "        # Initialize merged dataframe\n",
    "        merged_df = None\n",
    "\n",
    "        for i, table in enumerate(tables):\n",
    "            df = table.df\n",
    "            print(f\"  Page table {i+1}: Shape {df.shape}\")\n",
    "\n",
    "            if i == 0:\n",
    "                # First table - keep as is\n",
    "                merged_df = df\n",
    "                if keep_header_from_first_page:\n",
    "                    # Store the header for comparison\n",
    "                    header_row = df.iloc[0].astype(str).str.strip()\n",
    "            else:\n",
    "                # Subsequent tables\n",
    "                if keep_header_from_first_page:\n",
    "                    # Check if first row matches the header pattern\n",
    "                    current_first_row = df.iloc[0].astype(str).str.strip()\n",
    "\n",
    "                    # If first row looks like a header (matches the original header), skip it\n",
    "                    if current_first_row.equals(header_row):\n",
    "                        df = df.iloc[1:]  # Skip the header row\n",
    "                        print(f\"    Skipped duplicate header row\")\n",
    "\n",
    "                # Append to merged dataframe\n",
    "                merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "        # Clean up the merged dataframe\n",
    "        # Option 1: If first row contains headers, set it as column names\n",
    "        if keep_header_from_first_page and not merged_df.empty:\n",
    "            # Check if the first row looks like headers (you can customize this logic)\n",
    "            first_row = merged_df.iloc[0].astype(str)\n",
    "            if all(first_row.str.len() > 0):  # Basic check for non-empty headers\n",
    "                merged_df.columns = first_row\n",
    "                merged_df = merged_df[1:].reset_index(drop=True)\n",
    "                print(\"\\nSet first row as column headers\")\n",
    "\n",
    "        # Remove any completely empty rows\n",
    "        merged_df = merged_df.dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nMerged table saved to: {output_file}\")\n",
    "        print(f\"Final table shape: {merged_df.shape}\")\n",
    "\n",
    "        # Display preview\n",
    "        print(\"\\nPreview of merged table:\")\n",
    "        print(merged_df.head(10))\n",
    "\n",
    "        return merged_df, tables\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def merge_with_custom_header_handling(\n",
    "    pdf_path,\n",
    "    pages=\"all\",\n",
    "    flavor=\"lattice\",\n",
    "    output_file=\"merged_table.csv\",\n",
    "    header_rows_to_skip=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Alternative method: Skip a specific number of rows (headers) from each page except the first\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pdf_path : str\n",
    "        Path to the PDF file\n",
    "    pages : str\n",
    "        Pages to parse\n",
    "    flavor : str\n",
    "        'lattice' or 'stream'\n",
    "    output_file : str\n",
    "        Output CSV filename\n",
    "    header_rows_to_skip : int\n",
    "        Number of header rows to skip on pages after the first\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Reading tables from {pdf_path}...\")\n",
    "        tables = camelot.read_pdf(pdf_path, pages=pages, flavor=flavor)\n",
    "\n",
    "        if len(tables) == 0:\n",
    "            print(\"No tables found\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Found {len(tables)} table(s)\")\n",
    "\n",
    "        # Collect all dataframes\n",
    "        dfs = []\n",
    "        for i, table in enumerate(tables):\n",
    "            df = table.df\n",
    "\n",
    "            if i == 0:\n",
    "                # Keep the first page as is\n",
    "                dfs.append(df)\n",
    "                print(f\"  Page {i+1}: Keeping all {len(df)} rows\")\n",
    "            else:\n",
    "                # Skip header rows on subsequent pages\n",
    "                df_trimmed = df.iloc[header_rows_to_skip:]\n",
    "                dfs.append(df_trimmed)\n",
    "                print(\n",
    "                    f\"  Page {i+1}: Skipping {header_rows_to_skip} header row(s), keeping {len(df_trimmed)} rows\"\n",
    "                )\n",
    "\n",
    "        # Merge all dataframes\n",
    "        merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Clean up\n",
    "        merged_df = merged_df.dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nSaved merged table to: {output_file}\")\n",
    "        print(f\"Total rows: {len(merged_df)}\")\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db31628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tables from oficjalny_spis_pna_2025.pdf...\n",
      "Found 18 table(s) across pages\n",
      "  Page table 1: Shape (75, 6)\n",
      "  Page table 2: Shape (83, 6)\n",
      "  Page table 3: Shape (83, 6)\n",
      "  Page table 4: Shape (83, 6)\n",
      "  Page table 5: Shape (83, 7)\n",
      "  Page table 6: Shape (82, 6)\n",
      "  Page table 7: Shape (83, 6)\n",
      "  Page table 8: Shape (83, 6)\n",
      "  Page table 9: Shape (83, 6)\n",
      "  Page table 10: Shape (83, 7)\n",
      "  Page table 11: Shape (83, 6)\n",
      "  Page table 12: Shape (83, 6)\n",
      "  Page table 13: Shape (83, 6)\n",
      "  Page table 14: Shape (83, 7)\n",
      "  Page table 15: Shape (83, 7)\n",
      "  Page table 16: Shape (83, 6)\n",
      "  Page table 17: Shape (83, 6)\n",
      "  Page table 18: Shape (83, 7)\n",
      "\n",
      "Merged table saved to: merged_output.csv\n",
      "Final table shape: (1485, 7)\n",
      "\n",
      "Preview of merged table:\n",
      "        0                        1       2          3             4  \\\n",
      "0          PNA miejscowości i ulic                                    \n",
      "1     PNA       Miejscowość\\nUlica  Numery      Gmina        Powiat   \n",
      "2  83-440                 Abisynia             Karsin    kościerski   \n",
      "3  20-388     Abramowice Kościelne              Głusk      lubelski   \n",
      "4  20-388      Abramowice Prywatne              Głusk      lubelski   \n",
      "5  23-450                  Abramów              Goraj   biłgorajski   \n",
      "6  21-143                  Abramów            Abramów   lubartowski   \n",
      "7  05-310                   Abramy           Kałuszyn        miński   \n",
      "8  16-123               Achrymowce            Kuźnica      sokólski   \n",
      "9  27-640              Adamczowice          Klimontów  sandomierski   \n",
      "\n",
      "                5    6  \n",
      "0                  NaN  \n",
      "1     Województwo  NaN  \n",
      "2       pomorskie  NaN  \n",
      "3       lubelskie  NaN  \n",
      "4       lubelskie  NaN  \n",
      "5       lubelskie  NaN  \n",
      "6       lubelskie  NaN  \n",
      "7     mazowieckie  NaN  \n",
      "8       podlaskie  NaN  \n",
      "9  świętokrzyskie  NaN  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df, tables = merge_multipage_table_to_csv(\n",
    "        pdf_path=\"oficjalny_spis_pna_2025.pdf\",\n",
    "        pages=\"3-20\",\n",
    "        flavor=\"stream\",\n",
    "        output_file=\"merged_output.csv\",\n",
    "        keep_header_from_first_page=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8a2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postal-codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
